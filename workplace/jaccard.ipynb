{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from os import listdir\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer= PorterStemmer()\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def text_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_empty_lines(text):\n",
    "    return text.replace('\\\\n', ' ')\n",
    "\n",
    "def remove_ending_blocks(text):\n",
    "    # target_block = ['== See also ==','== References ==','== External links ==','=== Notable people ===']\n",
    "    target_block = ['== See also ==','== References ==','== External links ==']\n",
    "    for target in target_block:\n",
    "        start_i = text.find(target)\n",
    "        if start_i != -1:\n",
    "            text = text[:start_i]\n",
    "        \n",
    "    return text\n",
    "\n",
    "def remove_block_title(text):\n",
    "    return re.sub(r'[=]+\\s[a-z\\s]+\\s[=]+', '', text)\n",
    "\n",
    "def remove_extra_blanks(text):\n",
    "    return re.sub(' {2,}', ' ', text)\n",
    "\n",
    "def remove_punc(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def remove_spec_char(text):\n",
    "    return re.sub(r'[^A-Za-z0-9\\s]+', '', text)\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return [i for i in tokens if not i in stop_words]\n",
    "\n",
    "def stemming(word_list):\n",
    "    return [stemmer.stem(word) for word in word_list]\n",
    "\n",
    "def lemmatize(word_list):\n",
    "    return [lemmatizer.lemmatize(word) for word in word_list]\n",
    "\n",
    "def remove_city_name(word_list, name):\n",
    "    return [word for word in word_list if word != name]\n",
    "\n",
    "# Preprocess the sample documents:\n",
    "def preprocess(doc, city_name):\n",
    "    # Remove several ending blocks\n",
    "    doc = remove_ending_blocks(doc)\n",
    "    # Lower the letters\n",
    "    doc = text_lower(doc)\n",
    "    # Remove empty lines\n",
    "    doc = remove_empty_lines(doc)\n",
    "\n",
    "    # Remove block titles:\n",
    "    doc = remove_block_title(doc)\n",
    "    # Remove extra blank spaces:\n",
    "    doc = remove_extra_blanks(doc)\n",
    "    # Remove punctuations:\n",
    "    doc = remove_punc(doc)\n",
    "    # Remove speacial characters:\n",
    "    doc = remove_spec_char(doc)\n",
    "\n",
    "    # Tokenization:\n",
    "    tokens = tokenize(doc)\n",
    "    # Stemming:\n",
    "    # tokens = stemming(tokens)\n",
    "    # Lemmatizing:\n",
    "    tokens = lemmatize(tokens)\n",
    "    # Remove city names:\n",
    "    tokens = remove_city_name(tokens, city_name)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------football------------------\n",
      "0.5571741306624307\n",
      "------------------castle------------------\n",
      "0.6396587213360692\n",
      "------------------shopping------------------\n",
      "0.6396587213360692\n",
      "------------------monument------------------\n",
      "0.7094859686180036\n",
      "------------------forest------------------\n",
      "0.5571741306624307\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from sklearn.metrics import ndcg_score, dcg_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as geek \n",
    "\n",
    "def get_jaccard_score(tokenized_query,tokenized_corpus):\n",
    "    jaccard_scores = np.array([])\n",
    "    for one_doc in tokenized_corpus:\n",
    "        intersection = geek.intersect1d(tokenized_query, one_doc)\n",
    "        union = geek.union1d(tokenized_query, one_doc)\n",
    "        jaccard_scores = np.append(jaccard_scores, float(len(intersection)) / len(union))\n",
    "    return jaccard_scores\n",
    "\n",
    "def argsort(seq):\n",
    "    return sorted(range(len(seq)), key=seq.__getitem__, reverse=True)\n",
    "\n",
    "# For sampling use: get the sampling documents\n",
    "doc_filenames = sorted(listdir('../data/sample_docs'))\n",
    "sample_docs = []\n",
    "city_country_names = []\n",
    "for i in range(len(doc_filenames)):\n",
    "    line = doc_filenames[i].split('_')\n",
    "    city_country_names.append((line[0], line[1]))\n",
    "    with open('../data/sample_docs/' + doc_filenames[i], 'r', encoding='utf-8') as df:\n",
    "        doc = df.read()\n",
    "        sample_docs.append(doc)\n",
    "\n",
    "tokenized_corpus = []\n",
    "\n",
    "for ind1, doc in enumerate(sample_docs):\n",
    "    doc = str(doc)\n",
    "    name = city_country_names[ind1][0].lower()\n",
    "    # print(name)\n",
    "    tokens = preprocess(doc, name)\n",
    "    tokenized_corpus.append(tokens)\n",
    "\n",
    "\n",
    "top_10_test_queries = [[\"waterfall\"], [\"silk\"], [\"desert\"],[\"volcano\"],[\"beer\"],[\"coconut\"],[\"seafood\"]]\n",
    "top_100_test_queries = [[\"football\"], [\"castle\"], [\"shopping\"], ['monument'], [\"forest\"]]\n",
    "\n",
    "\n",
    "for q in top_100_test_queries:\n",
    "    print(\"------------------{}------------------\".format(q[0]))\n",
    "    \n",
    "    tokenized_query = q\n",
    "    jaccard_scores = get_jaccard_score(tokenized_query,tokenized_corpus)\n",
    "    top_n = []\n",
    "    top_n.append([city_country_names[i] for i in argsort(jaccard_scores)])\n",
    "    top_n = top_n[0][:100]\n",
    "    # print([(pair[0].encode('utf-8'), pair[1].encode('utf-8'))for pair in top_n])\n",
    "\n",
    "    # NDCG evaluations:\n",
    "    relevance_score = [1 for i in range(100)]\n",
    "\n",
    "    annotate_result = pd.read_csv('../data/annotate_result.csv', encoding='utf-8', header=0)\n",
    "\n",
    "    true_relevence = []\n",
    "    for top_city in top_n:\n",
    "        true_relevence.append(annotate_result[annotate_result['city'] == str((top_city[0], top_city[1]))].squeeze()[q[0]])\n",
    "\n",
    "    # Releveance scores in Ideal order \n",
    "    true_relevance = np.asarray([true_relevence]) \n",
    "    \n",
    "    # Releveance scores in output order \n",
    "    relevance_score = np.asarray([relevance_score]) \n",
    "\n",
    "    print(ndcg_score( true_relevance, relevance_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SI650 Homework 2 - Covid Ranking.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
